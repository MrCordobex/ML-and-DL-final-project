{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe2d23d",
   "metadata": {},
   "source": [
    "# Arquitecturas h√≠bridas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe436e02",
   "metadata": {},
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "013ac7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, cohen_kappa_score, confusion_matrix, classification_report)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ea1b9",
   "metadata": {},
   "source": [
    "## Fusi√≥n temprana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06cdb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cpu\n",
      "\n",
      "üß† Cargando ResNet101 preentrenada...\n",
      "üìÇ Dataset cargado: 802 muestras\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURACI√ìN ---\n",
    "# Detectamos si tienes GPU para que la extracci√≥n de caracter√≠sticas vuele\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Usando dispositivo: {device}\")\n",
    "\n",
    "# Rutas\n",
    "csv_path = 'Datos/metadata.csv'\n",
    "images_dir = 'Datos/images'\n",
    "\n",
    "# ==========================================\n",
    "# 1. PREPARACI√ìN DE LA CNN (ResNet101)\n",
    "# ==========================================\n",
    "print(\"\\nüß† Cargando ResNet101 preentrenada...\")\n",
    "\n",
    "# Funci√≥n para obtener el modelo extractor de caracter√≠sticas\n",
    "def get_feature_extractor():\n",
    "    # Cargamos ResNet101 con pesos de ImageNet\n",
    "    model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "    \n",
    "    # TRUCO: Quitamos la √∫ltima capa (fc) que es la que clasifica (perro, gato...)\n",
    "    # La sustituimos por una identidad para que pase el vector directo\n",
    "    # ResNet101 saca un vector de 2048 caracter√≠sticas antes de la clasificaci√≥n\n",
    "    model.fc = nn.Identity()\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval() # Modo evaluaci√≥n (congela dropout, etc)\n",
    "    return model\n",
    "\n",
    "cnn_extractor = get_feature_extractor()\n",
    "\n",
    "# Transformaciones necesarias para que ResNet entienda las fotos\n",
    "preprocess = transforms.Compose([\n",
    "    # ALERTA: Pasamos una TUPLA (224, 224).\n",
    "    # Esto aplasta o estira la imagen para que encaje exacta, sin recortar bordes.\n",
    "    transforms.Resize((224, 224)), \n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ==========================================\n",
    "# 2. CARGA DE DATOS E IM√ÅGENES\n",
    "# ==========================================\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Reconstruimos la ruta: dataset_final/images/CLASE/FOTO.png\n",
    "        img_name = row['img_id']\n",
    "        label_name = row['diagnostic']\n",
    "        img_path = os.path.join(self.root_dir, label_name, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            # Si falla una imagen, creamos una negra (no deber√≠a pasar si limpiamos bien)\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image\n",
    "\n",
    "# Leemos el CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"üìÇ Dataset cargado: {len(df)} muestras\")\n",
    "\n",
    "# Dataset y DataLoader para ir por lotes\n",
    "dataset = SkinLesionDataset(df, images_dir, transform=preprocess)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False) # Shuffle False para mantener orden con CSV!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f400d74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì∏ Extrayendo caracter√≠sticas visuales (Esto puede tardar un poco)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [01:47<00:00,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracci√≥n completada. Dimensi√≥n de features visuales: (802, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 3. EXTRACCI√ìN DE VECTORES (EMBEDDINGS)\n",
    "# ==========================================\n",
    "print(\"\\nüì∏ Extrayendo caracter√≠sticas visuales (Esto puede tardar un poco)...\")\n",
    "\n",
    "features_list = []\n",
    "\n",
    "with torch.no_grad(): # No necesitamos gradientes, solo inferencia\n",
    "    for images in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        # Pasamos las fotos por la ResNet\n",
    "        outputs = cnn_extractor(images)\n",
    "        # Pasamos a CPU y numpy\n",
    "        features_list.append(outputs.cpu().numpy())\n",
    "\n",
    "# Unimos todos los bloques en una gran matriz de caracter√≠sticas\n",
    "# Tama√±o esperado: (N_muestras, 2048)\n",
    "image_features = np.concatenate(features_list, axis=0)\n",
    "print(f\"‚úÖ Extracci√≥n completada. Dimensi√≥n de features visuales: {image_features.shape}\")\n",
    "\n",
    "# Convertimos a DataFrame para unirlo bonito\n",
    "feat_cols = [f'img_feat_{i}' for i in range(image_features.shape[1])]\n",
    "df_features = pd.DataFrame(image_features, columns=feat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c56940ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Procesando datos tabulares...\n",
      "   - Features Tabulares: 66 columnas\n",
      "\n",
      "üîó FUSIONANDO (Early Fusion)...\n",
      "‚ú® Dataset H√≠brido Final: 802 filas x 2114 columnas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\AppData\\Local\\Temp\\ipykernel_30532\\1936509309.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_tab[col] = X_tab[col].fillna('DESCONOCIDO')\n",
      "C:\\Users\\pedro\\AppData\\Local\\Temp\\ipykernel_30532\\1936509309.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_tab[col] = X_tab[col].fillna(X_tab[col].mean())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4. PREPARACI√ìN DATOS TABULARES (Limpieza)\n",
    "# ==========================================\n",
    "print(\"\\n‚öôÔ∏è Procesando datos tabulares...\")\n",
    "\n",
    "# Hacemos copia para no tocar el original\n",
    "df_tab = df.copy()\n",
    "\n",
    "# Variables a ignorar\n",
    "ignore_cols = ['patient_id', 'lesion_id', 'img_id', 'diagnostic', 'biopsed', 'path'] # path si existiera\n",
    "cols_to_use = [c for c in df_tab.columns if c not in ignore_cols]\n",
    "\n",
    "X_tab = df_tab[cols_to_use]\n",
    "y = df_tab['diagnostic']\n",
    "\n",
    "# Limpieza (Rellenar nulos y One-Hot Encoding)\n",
    "# 1. Rellenar\n",
    "for col in X_tab.columns:\n",
    "    if X_tab[col].dtype == 'object':\n",
    "        X_tab[col] = X_tab[col].fillna('DESCONOCIDO')\n",
    "    else:\n",
    "        X_tab[col] = X_tab[col].fillna(X_tab[col].mean())\n",
    "\n",
    "# 2. Convertir texto a n√∫meros (One-Hot)\n",
    "X_tab = pd.get_dummies(X_tab, drop_first=True)\n",
    "\n",
    "print(f\"   - Features Tabulares: {X_tab.shape[1]} columnas\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. FUSI√ìN (CONCATENACI√ìN)\n",
    "# ==========================================\n",
    "print(\"\\nüîó FUSIONANDO (Early Fusion)...\")\n",
    "\n",
    "# Reseteamos √≠ndices para evitar problemas al concatenar\n",
    "X_tab.reset_index(drop=True, inplace=True)\n",
    "df_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Unimos horizontalmente: [Datos Tabulares | Vector ResNet]\n",
    "X_final = pd.concat([X_tab, df_features], axis=1)\n",
    "\n",
    "print(f\"‚ú® Dataset H√≠brido Final: {X_final.shape[0]} filas x {X_final.shape[1]} columnas\")\n",
    "# Codificamos la etiqueta (Target) a n√∫meros (0, 1, 2...)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca374a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèãÔ∏è Entrenando Gradient Boosting (Esto es potente)...\n",
      "\n",
      "üìä --- RESULTADOS DE LA EVALUACI√ìN ---\n",
      "‚úÖ Accuracy (Exactitud):  0.6025\n",
      "‚úÖ Precision (Precisi√≥n): 0.6063\n",
      "‚úÖ Recall (Sensibilidad): 0.6025\n",
      "‚úÖ F1-Score:              0.5962\n",
      "‚úÖ Kappa de Cohen:        0.5132\n",
      "\n",
      "üîç Matriz de Confusi√≥n:\n",
      "[[21  5  0  1  3  0]\n",
      " [ 6 18  0  1  4  1]\n",
      " [ 1  0  2  2  4  2]\n",
      " [ 1  2  0 21  2  4]\n",
      " [ 3  8  1  0 18  0]\n",
      " [ 2  4  1  6  0 17]]\n",
      "\n",
      "üìã Reporte Detallado:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACK       0.62      0.70      0.66        30\n",
      "         BCC       0.49      0.60      0.54        30\n",
      "         MEL       0.50      0.18      0.27        11\n",
      "         NEV       0.68      0.70      0.69        30\n",
      "         SCC       0.58      0.60      0.59        30\n",
      "         SEK       0.71      0.57      0.63        30\n",
      "\n",
      "    accuracy                           0.60       161\n",
      "   macro avg       0.60      0.56      0.56       161\n",
      "weighted avg       0.61      0.60      0.60       161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 6. ENTRENAMIENTO Y EVALUACI√ìN\n",
    "# ==========================================\n",
    "print(\"\\nüèãÔ∏è Entrenando Gradient Boosting (Esto es potente)...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Usamos GradientBoostingClassifier de Sklearn (robusto y potente)\n",
    "# Si quisieras XGBoost espec√≠fico: import xgboost as xgb; clf = xgb.XGBClassifier(...)\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "#clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "#clf = RandomForestClassifier(n_estimators=400, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicci√≥n\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# ==========================================\n",
    "# 7. M√âTRICAS SOLICITADAS\n",
    "# ==========================================\n",
    "print(\"\\nüìä --- RESULTADOS DE LA EVALUACI√ìN ---\")\n",
    "\n",
    "# Calculamos m√©tricas\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "# 'weighted' calcula la media ponderada por el n√∫mero de muestras de cada clase (ideal para multiclass)\n",
    "prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "rec = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úÖ Accuracy (Exactitud):  {acc:.4f}\")\n",
    "print(f\"‚úÖ Precision (Precisi√≥n): {prec:.4f}\")\n",
    "print(f\"‚úÖ Recall (Sensibilidad): {rec:.4f}\")\n",
    "print(f\"‚úÖ F1-Score:              {f1:.4f}\")\n",
    "print(f\"‚úÖ Kappa de Cohen:        {kappa:.4f}\")\n",
    "\n",
    "print(\"\\nüîç Matriz de Confusi√≥n:\")\n",
    "print(conf_mat)\n",
    "\n",
    "print(\"\\nüìã Reporte Detallado:\")\n",
    "# Recuperamos los nombres reales de las clases para el reporte\n",
    "target_names = le.inverse_transform(sorted(list(set(y_test))))\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9cc033",
   "metadata": {},
   "source": [
    "## Fusi√≥n tard√≠a (Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0378cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, cohen_kappa_score, confusion_matrix, classification_report)\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6377153b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cpu\n",
      "\n",
      "‚úÇÔ∏è Dividiendo datos en Train (Modelos Base), Val (Meta-Modelo) y Test (Final)...\n",
      "Clases detectadas: ['ACK' 'BCC' 'MEL' 'NEV' 'SCC' 'SEK']\n",
      "   - Train set: 480 (Para entrenar ResNet, XGB, RF)\n",
      "   - Val set:   161   (Para entrenar al MLP Stacker)\n",
      "   - Test set:  161  (Para evaluaci√≥n final)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- CONFIGURACI√ìN ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Usando dispositivo: {device}\")\n",
    "\n",
    "csv_path = 'Datos/metadata.csv'\n",
    "images_dir = 'Datos/images'\n",
    "\n",
    "# ==========================================\n",
    "# 1. PREPARACI√ìN DE DATOS (SPLIT 3 V√çAS)\n",
    "# ==========================================\n",
    "print(\"\\n‚úÇÔ∏è Dividiendo datos en Train (Modelos Base), Val (Meta-Modelo) y Test (Final)...\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Codificamos las etiquetas a n√∫meros (0, 1, 2...)\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['diagnostic'])\n",
    "num_classes = len(le.classes_)\n",
    "print(f\"Clases detectadas: {le.classes_}\")\n",
    "\n",
    "# Split 1: Separamos Test (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    df, df['label_encoded'], test_size=0.2, random_state=42, stratify=df['label_encoded']\n",
    ")\n",
    "\n",
    "# Split 2: Del resto, separamos Validaci√≥n (20% del total original aprox)\n",
    "# Train ser√° el 60% del total, Val el 20%, Test el 20%\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"   - Train set: {len(X_train)} (Para entrenar ResNet, XGB, RF)\")\n",
    "print(f\"   - Val set:   {len(X_val)}   (Para entrenar al MLP Stacker)\")\n",
    "print(f\"   - Test set:  {len(X_test)}  (Para evaluaci√≥n final)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ffe46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Configurando ResNet101 para Fine-Tuning...\n",
      "üèãÔ∏è Entrenando ResNet (Fine-Tuning de la √∫ltima capa)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:34<00:00,  6.32s/it]\n",
      "Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:35<00:00,  6.35s/it]\n",
      "Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:47<00:00,  7.15s/it]\n",
      "Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:51<00:00,  7.46s/it]\n",
      "Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:47<00:00,  7.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ Generando predicciones de ResNet para Val y Test...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2. MODELO DE IMAGEN (RESNET101 - FINE TUNING)\n",
    "# ==========================================\n",
    "print(\"\\nüß† Configurando ResNet101 para Fine-Tuning...\")\n",
    "\n",
    "# Dataset Personalizado\n",
    "class SkinDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row['diagnostic'], row['img_id'])\n",
    "        label = row['label_encoded']\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (224, 224)) # Fallback\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Transformaciones\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(SkinDataset(X_train, images_dir, train_transforms), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(SkinDataset(X_val, images_dir, val_test_transforms), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(SkinDataset(X_test, images_dir, val_test_transforms), batch_size=32, shuffle=False)\n",
    "\n",
    "# Definir Modelo\n",
    "resnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "# Congelamos capas base (opcional, para ir m√°s r√°pido)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "# Cambiamos la capa final para clasificar nuestras clases\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# Entrenamiento R√°pido de ResNet\n",
    "optimizer = optim.Adam(resnet.fc.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"üèãÔ∏è Entrenando ResNet (Fine-Tuning de la √∫ltima capa)...\")\n",
    "resnet.train()\n",
    "epochs = 5 # Pocas √©pocas para demostraci√≥n\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "# Funci√≥n para obtener PROBABILIDADES\n",
    "def get_probs(model, loader):\n",
    "    model.eval()\n",
    "    probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # Aplicamos Softmax para tener probabilidades (0 a 1)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            probs_list.append(probs.cpu().numpy())\n",
    "    return np.concatenate(probs_list)\n",
    "\n",
    "print(\"üì∏ Generando predicciones de ResNet para Val y Test...\")\n",
    "resnet_probs_val = get_probs(resnet, val_loader)\n",
    "resnet_probs_test = get_probs(resnet, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "049463e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Entrenando Modelos Tabulares...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pedro\\anaconda4\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [15:03:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelos tabulares entrenados y predicciones generadas.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 3. MODELOS TABULARES (XGBOOST y RANDOM FOREST)\n",
    "# ==========================================\n",
    "print(\"\\nüìä Entrenando Modelos Tabulares...\")\n",
    "\n",
    "# Preprocesamiento Tabular\n",
    "def process_tabular(df_subset):\n",
    "    df_proc = df_subset.drop(columns=['patient_id', 'lesion_id', 'img_id', 'diagnostic', 'path', 'label_encoded', 'biopsed'], errors='ignore')\n",
    "    # Rellenar y One-Hot\n",
    "    for col in df_proc.columns:\n",
    "        if df_proc[col].dtype == 'object':\n",
    "            df_proc[col] = df_proc[col].fillna('DESCONOCIDO')\n",
    "        else:\n",
    "            df_proc[col] = df_proc[col].fillna(df_proc[col].mean())\n",
    "    return pd.get_dummies(df_proc, drop_first=True)\n",
    "\n",
    "# Procesamos todos juntos para alinear columnas, luego separamos\n",
    "df_all_tab = pd.concat([X_train, X_val, X_test])\n",
    "df_all_proc = process_tabular(df_all_tab)\n",
    "\n",
    "# Recuperamos los splits procesados\n",
    "tab_train = df_all_proc.iloc[:len(X_train)]\n",
    "tab_val = df_all_proc.iloc[len(X_train):len(X_train)+len(X_val)]\n",
    "tab_test = df_all_proc.iloc[len(X_train)+len(X_val):]\n",
    "\n",
    "# 3.1 Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=42)\n",
    "rf.fit(tab_train, y_train)\n",
    "rf_probs_val = rf.predict_proba(tab_val)\n",
    "rf_probs_test = rf.predict_proba(tab_test)\n",
    "\n",
    "# 3.2 XGBoost\n",
    "xgb = XGBClassifier(n_estimators=400, random_state=42, eval_metric='mlogloss', use_label_encoder=False)\n",
    "xgb.fit(tab_train, y_train)\n",
    "xgb_probs_val = xgb.predict_proba(tab_val)\n",
    "xgb_probs_test = xgb.predict_proba(tab_test)\n",
    "\n",
    "print(\"‚úÖ Modelos tabulares entrenados y predicciones generadas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "402dbaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Construyendo el Meta-Modelo (Stacking con MLP)...\n",
      "   - Dimensiones entrada Stacking: (161, 18)\n",
      "üîÆ Prediciendo resultado final con el Stacking...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4. STACKING (FUSI√ìN TARD√çA CON MLP)\n",
    "# ==========================================\n",
    "print(\"\\nüîó Construyendo el Meta-Modelo (Stacking con MLP)...\")\n",
    "\n",
    "# Construimos la entrada para el meta-modelo concatenando las probabilidades\n",
    "# Input = [Probs ResNet (6 cols) | Probs RF (6 cols) | Probs XGB (6 cols)]\n",
    "X_stack_val = np.hstack([resnet_probs_val, rf_probs_val, xgb_probs_val])\n",
    "X_stack_test = np.hstack([resnet_probs_test, rf_probs_test, xgb_probs_test])\n",
    "\n",
    "print(f\"   - Dimensiones entrada Stacking: {X_stack_val.shape}\")\n",
    "\n",
    "# Usamos un MLPClassifier (Perceptr√≥n Multicapa) como meta-aprendiz\n",
    "# Es el \"Juez\" que decide a qui√©n creer\n",
    "meta_model = MLPClassifier(hidden_layer_sizes=(128, 64, 32), activation='relu', solver='adam', max_iter=2000, random_state=42)\n",
    "meta_model.fit(X_stack_val, y_val)\n",
    "\n",
    "print(\"üîÆ Prediciendo resultado final con el Stacking...\")\n",
    "y_final_pred = meta_model.predict(X_stack_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79b671e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ --- RESULTADOS FINALES (STACKING) ---\n",
      "‚úÖ Accuracy:        0.6273\n",
      "‚úÖ Precision:       0.6322\n",
      "‚úÖ Recall:          0.6273\n",
      "‚úÖ F1-Score:        0.6223\n",
      "‚úÖ Kappa de Cohen:  0.5440\n",
      "\n",
      "üîç Matriz de Confusi√≥n:\n",
      "[[23  3  0  1  1  2]\n",
      " [ 4 15  0  0  9  2]\n",
      " [ 1  2  4  2  0  2]\n",
      " [ 3  1  0 22  0  4]\n",
      " [ 5  9  0  0 15  1]\n",
      " [ 0  1  1  5  1 22]]\n",
      "\n",
      "üìã Reporte por Clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACK       0.64      0.77      0.70        30\n",
      "         BCC       0.48      0.50      0.49        30\n",
      "         MEL       0.80      0.36      0.50        11\n",
      "         NEV       0.73      0.73      0.73        30\n",
      "         SCC       0.58      0.50      0.54        30\n",
      "         SEK       0.67      0.73      0.70        30\n",
      "\n",
      "    accuracy                           0.63       161\n",
      "   macro avg       0.65      0.60      0.61       161\n",
      "weighted avg       0.63      0.63      0.62       161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 5. EVALUACI√ìN FINAL\n",
    "# ==========================================\n",
    "print(\"\\nüèÜ --- RESULTADOS FINALES (STACKING) ---\")\n",
    "\n",
    "# Calculamos m√©tricas\n",
    "acc = accuracy_score(y_test, y_final_pred)\n",
    "prec = precision_score(y_test, y_final_pred, average='weighted', zero_division=0)\n",
    "rec = recall_score(y_test, y_final_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_final_pred, average='weighted')\n",
    "kappa = cohen_kappa_score(y_test, y_final_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_final_pred)\n",
    "\n",
    "print(f\"‚úÖ Accuracy:        {acc:.4f}\")\n",
    "print(f\"‚úÖ Precision:       {prec:.4f}\")\n",
    "print(f\"‚úÖ Recall:          {rec:.4f}\")\n",
    "print(f\"‚úÖ F1-Score:        {f1:.4f}\")\n",
    "print(f\"‚úÖ Kappa de Cohen:  {kappa:.4f}\")\n",
    "\n",
    "print(\"\\nüîç Matriz de Confusi√≥n:\")\n",
    "print(conf_mat)\n",
    "\n",
    "print(\"\\nüìã Reporte por Clase:\")\n",
    "print(classification_report(y_test, y_final_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e13465",
   "metadata": {},
   "source": [
    "## Red Neuronal Mixta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8ab0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "419f3a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cpu\n",
      "\n",
      "‚öôÔ∏è Procesando datos...\n",
      "   - Features Tabulares: 66\n",
      "   - Clases: ['ACK' 'BCC' 'MEL' 'NEV' 'SCC' 'SEK']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- CONFIGURACI√ìN ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Usando dispositivo: {device}\")\n",
    "\n",
    "csv_path = 'Datos/metadata.csv'\n",
    "images_dir = 'Datos/images'\n",
    "\n",
    "# ==========================================\n",
    "# 1. PREPARACI√ìN DE DATOS\n",
    "# ==========================================\n",
    "print(\"\\n‚öôÔ∏è Procesando datos...\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# A) Limpieza Tabular y One-Hot\n",
    "# Quitamos columnas que no son features (ID, paths, etc)\n",
    "cols_ignore = ['patient_id', 'lesion_id', 'img_id', 'diagnostic', 'path', 'biopsed']\n",
    "# Seleccionamos columnas tabulares\n",
    "tab_df = df.drop(columns=[c for c in cols_ignore if c in df.columns], errors='ignore').copy()\n",
    "\n",
    "# Rellenar Nulos\n",
    "for col in tab_df.columns:\n",
    "    if tab_df[col].dtype == 'object':\n",
    "        tab_df[col] = tab_df[col].fillna('DESCONOCIDO')\n",
    "    else:\n",
    "        tab_df[col] = tab_df[col].fillna(tab_df[col].mean())\n",
    "\n",
    "# One-Hot Encoding\n",
    "tab_df = pd.get_dummies(tab_df, drop_first=True)\n",
    "\n",
    "# IMPORTANTE: Escalar datos num√©ricos (Neural Networks aman el rango 0-1 o -1 a 1)\n",
    "scaler = StandardScaler()\n",
    "X_tab_scaled = scaler.fit_transform(tab_df)\n",
    "\n",
    "# B) Preparar Target\n",
    "le = LabelEncoder()\n",
    "y_labels = le.fit_transform(df['diagnostic'])\n",
    "num_classes = len(le.classes_)\n",
    "num_tab_features = X_tab_scaled.shape[1]\n",
    "\n",
    "print(f\"   - Features Tabulares: {num_tab_features}\")\n",
    "print(f\"   - Clases: {le.classes_}\")\n",
    "\n",
    "# C) Split Train/Val/Test\n",
    "# Creamos √≠ndices para dividir\n",
    "indices = np.arange(len(df))\n",
    "idx_train_val, idx_test, y_train_val, y_test = train_test_split(indices, y_labels, test_size=0.2, stratify=y_labels, random_state=42)\n",
    "idx_train, idx_val, y_train, y_val = train_test_split(idx_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86672585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2. DATASET MIXTO PERSONALIZADO\n",
    "# ==========================================\n",
    "class MixedDataset(Dataset):\n",
    "    def __init__(self, dataframe, tabular_data, labels, indices, root_dir, transform=None):\n",
    "        self.df = dataframe.iloc[indices].reset_index(drop=True)\n",
    "        self.tab_data = tabular_data[indices]\n",
    "        self.labels = labels[indices]\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Imagen\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row['diagnostic'], row['img_id'])\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # 2. Tabular (Convertir a float32 para PyTorch)\n",
    "        tab_input = torch.tensor(self.tab_data[idx], dtype=torch.float32)\n",
    "        \n",
    "        # 3. Label (Convertir a long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return image, tab_input, label\n",
    "\n",
    "# Transformaci√≥n \"Squish\" (Sin recorte)\n",
    "transforms_squish = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # <--- La clave para no recortar\n",
    "    transforms.RandomHorizontalFlip(), # Data augmentation suave\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# DataLoaders\n",
    "train_ds = MixedDataset(df, X_tab_scaled, y_labels, idx_train, images_dir, transforms_squish)\n",
    "val_ds = MixedDataset(df, X_tab_scaled, y_labels, idx_val, images_dir, val_transforms)\n",
    "test_ds = MixedDataset(df, X_tab_scaled, y_labels, idx_test, images_dir, val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# ==========================================\n",
    "# 3. ARQUITECTURA DE LA RED NEURONAL MIXTA\n",
    "# ==========================================\n",
    "class MixedNetwork(nn.Module):\n",
    "    def __init__(self, num_tab_cols, num_classes):\n",
    "        super(MixedNetwork, self).__init__()\n",
    "        \n",
    "        # RAMA 1: IMAGEN (ResNet18)\n",
    "        # Usamos ResNet18 porque es m√°s ligera que la 101 y entrena m√°s r√°pido end-to-end\n",
    "        self.cnn = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        num_cnn_features = self.cnn.fc.in_features # Suele ser 512\n",
    "        # Quitamos la capa clasificadora original\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        \n",
    "        # RAMA 2: TABULAR (MLP peque√±o)\n",
    "        self.tab_branch = nn.Sequential(\n",
    "            nn.Linear(num_tab_cols, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # FUSI√ìN (CONCATENACI√ìN)\n",
    "        # 512 (Imagen) + 64 (Tabular) = 576\n",
    "        combined_features = num_cnn_features + 64\n",
    "        \n",
    "        # CABEZA CLASIFICADORA FINAL\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes) # Salida final\n",
    "        )\n",
    "\n",
    "    def forward(self, image, tab_data):\n",
    "        # Procesar imagen\n",
    "        x_img = self.cnn(image) # (Batch, 512)\n",
    "        \n",
    "        # Procesar tabular\n",
    "        x_tab = self.tab_branch(tab_data) # (Batch, 64)\n",
    "        \n",
    "        # Concatenar\n",
    "        x_combined = torch.cat((x_img, x_tab), dim=1) # (Batch, 576)\n",
    "        \n",
    "        # Clasificar\n",
    "        output = self.classifier(x_combined)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adfc1284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Inicializando Modelo End-to-End...\n",
      "\n",
      "üî• INICIANDO FINETUNING BESTIA (10 √âpocas)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:01<00:00,  4.08s/it, loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train Loss: 1.6974 | Train Acc: 30.83%\n",
      "   Val Loss:   1.5779 | Val Acc:   36.65%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:59<00:00,  3.94s/it, loss=1.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train Loss: 1.1570 | Train Acc: 63.54%\n",
      "   Val Loss:   1.3856 | Val Acc:   43.48%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:57<00:00,  3.86s/it, loss=0.749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train Loss: 0.8721 | Train Acc: 77.50%\n",
      "   Val Loss:   1.3006 | Val Acc:   49.07%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:59<00:00,  3.98s/it, loss=0.589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train Loss: 0.6566 | Train Acc: 85.83%\n",
      "   Val Loss:   1.2904 | Val Acc:   47.20%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:57<00:00,  3.84s/it, loss=0.481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train Loss: 0.4888 | Train Acc: 92.92%\n",
      "   Val Loss:   1.2479 | Val Acc:   47.20%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:58<00:00,  3.87s/it, loss=0.454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train Loss: 0.3625 | Train Acc: 95.42%\n",
      "   Val Loss:   1.2605 | Val Acc:   50.93%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:58<00:00,  3.91s/it, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train Loss: 0.2730 | Train Acc: 98.12%\n",
      "   Val Loss:   1.2698 | Val Acc:   49.69%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:59<00:00,  3.97s/it, loss=0.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train Loss: 0.2022 | Train Acc: 98.75%\n",
      "   Val Loss:   1.2671 | Val Acc:   49.69%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:57<00:00,  3.82s/it, loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train Loss: 0.1489 | Train Acc: 99.58%\n",
      "   Val Loss:   1.2678 | Val Acc:   50.31%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:59<00:00,  3.99s/it, loss=0.0936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train Loss: 0.1150 | Train Acc: 99.58%\n",
      "   Val Loss:   1.2608 | Val Acc:   52.17%\n",
      "------------------------------------------------------------\n",
      "\n",
      "üèÜ Evaluando en conjunto de TEST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:10<00:00,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INFORME FINAL (Red Mixta End-to-End) ---\n",
      "Accuracy Global: 0.5093\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACK       0.52      0.47      0.49        30\n",
      "         BCC       0.37      0.43      0.40        30\n",
      "         MEL       0.50      0.36      0.42        11\n",
      "         NEV       0.54      0.70      0.61        30\n",
      "         SCC       0.45      0.33      0.38        30\n",
      "         SEK       0.67      0.67      0.67        30\n",
      "\n",
      "    accuracy                           0.51       161\n",
      "   macro avg       0.51      0.49      0.50       161\n",
      "weighted avg       0.51      0.51      0.50       161\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüß† Inicializando Modelo End-to-End...\")\n",
    "model = MixedNetwork(num_tab_features, num_classes).to(device)\n",
    "\n",
    "# Optimizador y Loss\n",
    "# LR bajo (1e-4) porque estamos haciendo finetuning de toda la red y no queremos romper los pesos de ResNet\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ==========================================\n",
    "# 4. BUCLE DE ENTRENAMIENTO (TRAINING LOOP)\n",
    "# ==========================================\n",
    "epochs = 10 # Ajusta seg√∫n paciencia\n",
    "\n",
    "print(f\"\\nüî• INICIANDO FINETUNING BESTIA ({epochs} √âpocas)...\")\n",
    "for epoch in range(epochs):\n",
    "    # --- TRAIN ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "    for images, tabs, labels in loop:\n",
    "        images, tabs, labels = images.to(device), tabs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, tabs) # Forward Pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward() # Backward Pass (Calcula gradientes para Imagen Y Tabular)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Actualizar barra de progreso\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss_train = train_loss / total_train\n",
    "    epoch_acc_train = correct_train / total_train\n",
    "\n",
    "    # --- VALIDATION ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, tabs, labels in val_loader:\n",
    "            images, tabs, labels = images.to(device), tabs.to(device), labels.to(device)\n",
    "            outputs = model(images, tabs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            \n",
    "    epoch_loss_val = val_loss / total_val\n",
    "    epoch_acc_val = correct_val / total_val\n",
    "\n",
    "    # --- PRINT STATS ---\n",
    "    print(f\"   Train Loss: {epoch_loss_train:.4f} | Train Acc: {epoch_acc_train:.2%}\")\n",
    "    print(f\"   Val Loss:   {epoch_loss_val:.4f} | Val Acc:   {epoch_acc_val:.2%}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# ==========================================\n",
    "# 5. EVALUACI√ìN FINAL (TEST)\n",
    "# ==========================================\n",
    "print(\"\\nüèÜ Evaluando en conjunto de TEST...\")\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, tabs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, tabs, labels = images.to(device), tabs.to(device), labels.to(device)\n",
    "        outputs = model(images, tabs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"\\n--- INFORME FINAL (Red Mixta End-to-End) ---\")\n",
    "print(f\"Accuracy Global: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
