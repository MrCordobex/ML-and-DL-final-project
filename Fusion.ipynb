{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe2d23d",
   "metadata": {},
   "source": [
    "# Arquitecturas h√≠bridas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe436e02",
   "metadata": {},
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "013ac7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, cohen_kappa_score, confusion_matrix, classification_report)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ea1b9",
   "metadata": {},
   "source": [
    "## Fusi√≥n temprana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c06cdb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cpu\n",
      "\n",
      "üß† Cargando ResNet101 preentrenada...\n",
      "üìÇ Dataset cargado: 802 muestras\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURACI√ìN ---\n",
    "# Detectamos si tienes GPU para que la extracci√≥n de caracter√≠sticas vuele\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Usando dispositivo: {device}\")\n",
    "\n",
    "# Rutas\n",
    "csv_path = 'Datos/metadata.csv'\n",
    "images_dir = 'Datos/images'\n",
    "\n",
    "# ==========================================\n",
    "# 1. PREPARACI√ìN DE LA CNN (ResNet101)\n",
    "# ==========================================\n",
    "print(\"\\nüß† Cargando ResNet101 preentrenada...\")\n",
    "\n",
    "# Funci√≥n para obtener el modelo extractor de caracter√≠sticas\n",
    "def get_feature_extractor():\n",
    "    # Cargamos ResNet101 con pesos de ImageNet\n",
    "    model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "    \n",
    "    # TRUCO: Quitamos la √∫ltima capa (fc) que es la que clasifica (perro, gato...)\n",
    "    # La sustituimos por una identidad para que pase el vector directo\n",
    "    # ResNet101 saca un vector de 2048 caracter√≠sticas antes de la clasificaci√≥n\n",
    "    model.fc = nn.Identity()\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval() # Modo evaluaci√≥n (congela dropout, etc)\n",
    "    return model\n",
    "\n",
    "cnn_extractor = get_feature_extractor()\n",
    "\n",
    "# Transformaciones necesarias para que ResNet entienda las fotos\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ==========================================\n",
    "# 2. CARGA DE DATOS E IM√ÅGENES\n",
    "# ==========================================\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Reconstruimos la ruta: dataset_final/images/CLASE/FOTO.png\n",
    "        img_name = row['img_id']\n",
    "        label_name = row['diagnostic']\n",
    "        img_path = os.path.join(self.root_dir, label_name, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            # Si falla una imagen, creamos una negra (no deber√≠a pasar si limpiamos bien)\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image\n",
    "\n",
    "# Leemos el CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"üìÇ Dataset cargado: {len(df)} muestras\")\n",
    "\n",
    "# Dataset y DataLoader para ir por lotes\n",
    "dataset = SkinLesionDataset(df, images_dir, transform=preprocess)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False) # Shuffle False para mantener orden con CSV!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f400d74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì∏ Extrayendo caracter√≠sticas visuales (Esto puede tardar un poco)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [01:41<00:00,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracci√≥n completada. Dimensi√≥n de features visuales: (802, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 3. EXTRACCI√ìN DE VECTORES (EMBEDDINGS)\n",
    "# ==========================================\n",
    "print(\"\\nüì∏ Extrayendo caracter√≠sticas visuales (Esto puede tardar un poco)...\")\n",
    "\n",
    "features_list = []\n",
    "\n",
    "with torch.no_grad(): # No necesitamos gradientes, solo inferencia\n",
    "    for images in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        # Pasamos las fotos por la ResNet\n",
    "        outputs = cnn_extractor(images)\n",
    "        # Pasamos a CPU y numpy\n",
    "        features_list.append(outputs.cpu().numpy())\n",
    "\n",
    "# Unimos todos los bloques en una gran matriz de caracter√≠sticas\n",
    "# Tama√±o esperado: (N_muestras, 2048)\n",
    "image_features = np.concatenate(features_list, axis=0)\n",
    "print(f\"‚úÖ Extracci√≥n completada. Dimensi√≥n de features visuales: {image_features.shape}\")\n",
    "\n",
    "# Convertimos a DataFrame para unirlo bonito\n",
    "feat_cols = [f'img_feat_{i}' for i in range(image_features.shape[1])]\n",
    "df_features = pd.DataFrame(image_features, columns=feat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c56940ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Procesando datos tabulares...\n",
      "   - Features Tabulares: 66 columnas\n",
      "\n",
      "üîó FUSIONANDO (Early Fusion)...\n",
      "‚ú® Dataset H√≠brido Final: 802 filas x 2114 columnas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\AppData\\Local\\Temp\\ipykernel_12936\\1936509309.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_tab[col] = X_tab[col].fillna('DESCONOCIDO')\n",
      "C:\\Users\\pedro\\AppData\\Local\\Temp\\ipykernel_12936\\1936509309.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_tab[col] = X_tab[col].fillna(X_tab[col].mean())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4. PREPARACI√ìN DATOS TABULARES (Limpieza)\n",
    "# ==========================================\n",
    "print(\"\\n‚öôÔ∏è Procesando datos tabulares...\")\n",
    "\n",
    "# Hacemos copia para no tocar el original\n",
    "df_tab = df.copy()\n",
    "\n",
    "# Variables a ignorar\n",
    "ignore_cols = ['patient_id', 'lesion_id', 'img_id', 'diagnostic', 'biopsed', 'path'] # path si existiera\n",
    "cols_to_use = [c for c in df_tab.columns if c not in ignore_cols]\n",
    "\n",
    "X_tab = df_tab[cols_to_use]\n",
    "y = df_tab['diagnostic']\n",
    "\n",
    "# Limpieza (Rellenar nulos y One-Hot Encoding)\n",
    "# 1. Rellenar\n",
    "for col in X_tab.columns:\n",
    "    if X_tab[col].dtype == 'object':\n",
    "        X_tab[col] = X_tab[col].fillna('DESCONOCIDO')\n",
    "    else:\n",
    "        X_tab[col] = X_tab[col].fillna(X_tab[col].mean())\n",
    "\n",
    "# 2. Convertir texto a n√∫meros (One-Hot)\n",
    "X_tab = pd.get_dummies(X_tab, drop_first=True)\n",
    "\n",
    "print(f\"   - Features Tabulares: {X_tab.shape[1]} columnas\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. FUSI√ìN (CONCATENACI√ìN)\n",
    "# ==========================================\n",
    "print(\"\\nüîó FUSIONANDO (Early Fusion)...\")\n",
    "\n",
    "# Reseteamos √≠ndices para evitar problemas al concatenar\n",
    "X_tab.reset_index(drop=True, inplace=True)\n",
    "df_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Unimos horizontalmente: [Datos Tabulares | Vector ResNet]\n",
    "X_final = pd.concat([X_tab, df_features], axis=1)\n",
    "\n",
    "print(f\"‚ú® Dataset H√≠brido Final: {X_final.shape[0]} filas x {X_final.shape[1]} columnas\")\n",
    "# Codificamos la etiqueta (Target) a n√∫meros (0, 1, 2...)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca374a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèãÔ∏è Entrenando Gradient Boosting (Esto es potente)...\n",
      "\n",
      "üìä --- RESULTADOS DE LA EVALUACI√ìN ---\n",
      "‚úÖ Accuracy (Exactitud):  0.5342\n",
      "‚úÖ Precision (Precisi√≥n): 0.5162\n",
      "‚úÖ Recall (Sensibilidad): 0.5342\n",
      "‚úÖ F1-Score:              0.5217\n",
      "‚úÖ Kappa de Cohen:        0.4275\n",
      "\n",
      "üîç Matriz de Confusi√≥n:\n",
      "[[14  5  0  0 11  0]\n",
      " [ 5 19  0  0  6  0]\n",
      " [ 3  1  0  3  3  1]\n",
      " [ 1  3  0 21  1  4]\n",
      " [ 7 11  0  1 11  0]\n",
      " [ 1  2  0  4  2 21]]\n",
      "\n",
      "üìã Reporte Detallado:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACK       0.45      0.47      0.46        30\n",
      "         BCC       0.46      0.63      0.54        30\n",
      "         MEL       0.00      0.00      0.00        11\n",
      "         NEV       0.72      0.70      0.71        30\n",
      "         SCC       0.32      0.37      0.34        30\n",
      "         SEK       0.81      0.70      0.75        30\n",
      "\n",
      "    accuracy                           0.53       161\n",
      "   macro avg       0.46      0.48      0.47       161\n",
      "weighted avg       0.52      0.53      0.52       161\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pedro\\anaconda4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pedro\\anaconda4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pedro\\anaconda4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 6. ENTRENAMIENTO Y EVALUACI√ìN\n",
    "# ==========================================\n",
    "print(\"\\nüèãÔ∏è Entrenando Gradient Boosting (Esto es potente)...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Usamos GradientBoostingClassifier de Sklearn (robusto y potente)\n",
    "# Si quisieras XGBoost espec√≠fico: import xgboost as xgb; clf = xgb.XGBClassifier(...)\n",
    "#clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "#clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "clf = RandomForestClassifier(n_estimators=400, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicci√≥n\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# ==========================================\n",
    "# 7. M√âTRICAS SOLICITADAS\n",
    "# ==========================================\n",
    "print(\"\\nüìä --- RESULTADOS DE LA EVALUACI√ìN ---\")\n",
    "\n",
    "# Calculamos m√©tricas\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "# 'weighted' calcula la media ponderada por el n√∫mero de muestras de cada clase (ideal para multiclass)\n",
    "prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "rec = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úÖ Accuracy (Exactitud):  {acc:.4f}\")\n",
    "print(f\"‚úÖ Precision (Precisi√≥n): {prec:.4f}\")\n",
    "print(f\"‚úÖ Recall (Sensibilidad): {rec:.4f}\")\n",
    "print(f\"‚úÖ F1-Score:              {f1:.4f}\")\n",
    "print(f\"‚úÖ Kappa de Cohen:        {kappa:.4f}\")\n",
    "\n",
    "print(\"\\nüîç Matriz de Confusi√≥n:\")\n",
    "print(conf_mat)\n",
    "\n",
    "print(\"\\nüìã Reporte Detallado:\")\n",
    "# Recuperamos los nombres reales de las clases para el reporte\n",
    "target_names = le.inverse_transform(sorted(list(set(y_test))))\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9cc033",
   "metadata": {},
   "source": [
    "## Fusi√≥n tard√≠a (Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0378cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, cohen_kappa_score, confusion_matrix, classification_report)\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6377153b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cpu\n",
      "\n",
      "‚úÇÔ∏è Dividiendo datos en Train (Modelos Base), Val (Meta-Modelo) y Test (Final)...\n",
      "Clases detectadas: ['ACK' 'BCC' 'MEL' 'NEV' 'SCC' 'SEK']\n",
      "   - Train set: 480 (Para entrenar ResNet, XGB, RF)\n",
      "   - Val set:   161   (Para entrenar al MLP Stacker)\n",
      "   - Test set:  161  (Para evaluaci√≥n final)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- CONFIGURACI√ìN ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Usando dispositivo: {device}\")\n",
    "\n",
    "csv_path = 'Datos/metadata.csv'\n",
    "images_dir = 'Datos/images'\n",
    "\n",
    "# ==========================================\n",
    "# 1. PREPARACI√ìN DE DATOS (SPLIT 3 V√çAS)\n",
    "# ==========================================\n",
    "print(\"\\n‚úÇÔ∏è Dividiendo datos en Train (Modelos Base), Val (Meta-Modelo) y Test (Final)...\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Codificamos las etiquetas a n√∫meros (0, 1, 2...)\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['diagnostic'])\n",
    "num_classes = len(le.classes_)\n",
    "print(f\"Clases detectadas: {le.classes_}\")\n",
    "\n",
    "# Split 1: Separamos Test (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    df, df['label_encoded'], test_size=0.2, random_state=42, stratify=df['label_encoded']\n",
    ")\n",
    "\n",
    "# Split 2: Del resto, separamos Validaci√≥n (20% del total original aprox)\n",
    "# Train ser√° el 60% del total, Val el 20%, Test el 20%\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"   - Train set: {len(X_train)} (Para entrenar ResNet, XGB, RF)\")\n",
    "print(f\"   - Val set:   {len(X_val)}   (Para entrenar al MLP Stacker)\")\n",
    "print(f\"   - Test set:  {len(X_test)}  (Para evaluaci√≥n final)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9ffe46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Configurando ResNet101 para Fine-Tuning...\n",
      "üèãÔ∏è Entrenando ResNet (Fine-Tuning de la √∫ltima capa)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:12<00:00,  4.83s/it]\n",
      "Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:13<00:00,  4.90s/it]\n",
      "Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:14<00:00,  4.96s/it]\n",
      "Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:15<00:00,  5.02s/it]\n",
      "Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:15<00:00,  5.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ Generando predicciones de ResNet para Val y Test...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2. MODELO DE IMAGEN (RESNET101 - FINE TUNING)\n",
    "# ==========================================\n",
    "print(\"\\nüß† Configurando ResNet101 para Fine-Tuning...\")\n",
    "\n",
    "# Dataset Personalizado\n",
    "class SkinDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row['diagnostic'], row['img_id'])\n",
    "        label = row['label_encoded']\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (224, 224)) # Fallback\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Transformaciones\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(256), transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize(256), transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(SkinDataset(X_train, images_dir, train_transforms), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(SkinDataset(X_val, images_dir, val_test_transforms), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(SkinDataset(X_test, images_dir, val_test_transforms), batch_size=32, shuffle=False)\n",
    "\n",
    "# Definir Modelo\n",
    "resnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "# Congelamos capas base (opcional, para ir m√°s r√°pido)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "# Cambiamos la capa final para clasificar nuestras clases\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# Entrenamiento R√°pido de ResNet\n",
    "optimizer = optim.Adam(resnet.fc.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"üèãÔ∏è Entrenando ResNet (Fine-Tuning de la √∫ltima capa)...\")\n",
    "resnet.train()\n",
    "epochs = 5 # Pocas √©pocas para demostraci√≥n\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "# Funci√≥n para obtener PROBABILIDADES\n",
    "def get_probs(model, loader):\n",
    "    model.eval()\n",
    "    probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # Aplicamos Softmax para tener probabilidades (0 a 1)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            probs_list.append(probs.cpu().numpy())\n",
    "    return np.concatenate(probs_list)\n",
    "\n",
    "print(\"üì∏ Generando predicciones de ResNet para Val y Test...\")\n",
    "resnet_probs_val = get_probs(resnet, val_loader)\n",
    "resnet_probs_test = get_probs(resnet, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "049463e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Entrenando Modelos Tabulares...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pedro\\anaconda4\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [14:27:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelos tabulares entrenados y predicciones generadas.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 3. MODELOS TABULARES (XGBOOST y RANDOM FOREST)\n",
    "# ==========================================\n",
    "print(\"\\nüìä Entrenando Modelos Tabulares...\")\n",
    "\n",
    "# Preprocesamiento Tabular\n",
    "def process_tabular(df_subset):\n",
    "    df_proc = df_subset.drop(columns=['patient_id', 'lesion_id', 'img_id', 'diagnostic', 'path', 'label_encoded', 'biopsed'], errors='ignore')\n",
    "    # Rellenar y One-Hot\n",
    "    for col in df_proc.columns:\n",
    "        if df_proc[col].dtype == 'object':\n",
    "            df_proc[col] = df_proc[col].fillna('DESCONOCIDO')\n",
    "        else:\n",
    "            df_proc[col] = df_proc[col].fillna(df_proc[col].mean())\n",
    "    return pd.get_dummies(df_proc, drop_first=True)\n",
    "\n",
    "# Procesamos todos juntos para alinear columnas, luego separamos\n",
    "df_all_tab = pd.concat([X_train, X_val, X_test])\n",
    "df_all_proc = process_tabular(df_all_tab)\n",
    "\n",
    "# Recuperamos los splits procesados\n",
    "tab_train = df_all_proc.iloc[:len(X_train)]\n",
    "tab_val = df_all_proc.iloc[len(X_train):len(X_train)+len(X_val)]\n",
    "tab_test = df_all_proc.iloc[len(X_train)+len(X_val):]\n",
    "\n",
    "# 3.1 Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(tab_train, y_train)\n",
    "rf_probs_val = rf.predict_proba(tab_val)\n",
    "rf_probs_test = rf.predict_proba(tab_test)\n",
    "\n",
    "# 3.2 XGBoost\n",
    "xgb = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False)\n",
    "xgb.fit(tab_train, y_train)\n",
    "xgb_probs_val = xgb.predict_proba(tab_val)\n",
    "xgb_probs_test = xgb.predict_proba(tab_test)\n",
    "\n",
    "print(\"‚úÖ Modelos tabulares entrenados y predicciones generadas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "402dbaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Construyendo el Meta-Modelo (Stacking con MLP)...\n",
      "   - Dimensiones entrada Stacking: (161, 18)\n",
      "üîÆ Prediciendo resultado final con el Stacking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pedro\\anaconda4\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4. STACKING (FUSI√ìN TARD√çA CON MLP)\n",
    "# ==========================================\n",
    "print(\"\\nüîó Construyendo el Meta-Modelo (Stacking con MLP)...\")\n",
    "\n",
    "# Construimos la entrada para el meta-modelo concatenando las probabilidades\n",
    "# Input = [Probs ResNet (6 cols) | Probs RF (6 cols) | Probs XGB (6 cols)]\n",
    "X_stack_val = np.hstack([resnet_probs_val, rf_probs_val, xgb_probs_val])\n",
    "X_stack_test = np.hstack([resnet_probs_test, rf_probs_test, xgb_probs_test])\n",
    "\n",
    "print(f\"   - Dimensiones entrada Stacking: {X_stack_val.shape}\")\n",
    "\n",
    "# Usamos un MLPClassifier (Perceptr√≥n Multicapa) como meta-aprendiz\n",
    "# Es el \"Juez\" que decide a qui√©n creer\n",
    "meta_model = MLPClassifier(hidden_layer_sizes=(32, 16), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "meta_model.fit(X_stack_val, y_val)\n",
    "\n",
    "print(\"üîÆ Prediciendo resultado final con el Stacking...\")\n",
    "y_final_pred = meta_model.predict(X_stack_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79b671e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ --- RESULTADOS FINALES (STACKING) ---\n",
      "‚úÖ Accuracy:        0.6708\n",
      "‚úÖ Precision:       0.6805\n",
      "‚úÖ Recall:          0.6708\n",
      "‚úÖ F1-Score:        0.6701\n",
      "‚úÖ Kappa de Cohen:  0.5980\n",
      "\n",
      "üîç Matriz de Confusi√≥n:\n",
      "[[23  4  0  1  1  1]\n",
      " [ 6 17  0  0  6  1]\n",
      " [ 0  0  6  2  1  2]\n",
      " [ 3  1  0 24  0  2]\n",
      " [ 5  8  0  0 17  0]\n",
      " [ 0  2  1  6  0 21]]\n",
      "\n",
      "üìã Reporte por Clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACK       0.62      0.77      0.69        30\n",
      "         BCC       0.53      0.57      0.55        30\n",
      "         MEL       0.86      0.55      0.67        11\n",
      "         NEV       0.73      0.80      0.76        30\n",
      "         SCC       0.68      0.57      0.62        30\n",
      "         SEK       0.78      0.70      0.74        30\n",
      "\n",
      "    accuracy                           0.67       161\n",
      "   macro avg       0.70      0.66      0.67       161\n",
      "weighted avg       0.68      0.67      0.67       161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 5. EVALUACI√ìN FINAL\n",
    "# ==========================================\n",
    "print(\"\\nüèÜ --- RESULTADOS FINALES (STACKING) ---\")\n",
    "\n",
    "# Calculamos m√©tricas\n",
    "acc = accuracy_score(y_test, y_final_pred)\n",
    "prec = precision_score(y_test, y_final_pred, average='weighted', zero_division=0)\n",
    "rec = recall_score(y_test, y_final_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_final_pred, average='weighted')\n",
    "kappa = cohen_kappa_score(y_test, y_final_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_final_pred)\n",
    "\n",
    "print(f\"‚úÖ Accuracy:        {acc:.4f}\")\n",
    "print(f\"‚úÖ Precision:       {prec:.4f}\")\n",
    "print(f\"‚úÖ Recall:          {rec:.4f}\")\n",
    "print(f\"‚úÖ F1-Score:        {f1:.4f}\")\n",
    "print(f\"‚úÖ Kappa de Cohen:  {kappa:.4f}\")\n",
    "\n",
    "print(\"\\nüîç Matriz de Confusi√≥n:\")\n",
    "print(conf_mat)\n",
    "\n",
    "print(\"\\nüìã Reporte por Clase:\")\n",
    "print(classification_report(y_test, y_final_pred, target_names=le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
